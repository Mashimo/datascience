{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364eda9a",
   "metadata": {},
   "source": [
    "# Markov-Shannon, a simple text generator\n",
    "Nowadays generating texts using Transformers neural network architectures reached high level of sophistication.  But the idea to generate text from existing examples is very old.  \n",
    "For example, a **Markov test generator** (the idea for this is apparently due to Claude Shannon who described it in his Communication Theory paper) is named after Andrey Markov (1856-1922), who probably never saw one (at the word level, at least). A Markov chain is essentially a finite-state automaton where all the transitions are assigned probabilities (I described them previously in the POS tagging series part).  \n",
    "It works very simply:  given a corpus of text, each word in the corpus becomes a state and each two-word sequence is a transition to another state. It’s assigned a probability determined by how many times it appears.   \n",
    "In his paper *A Mathematical Theory of Communication*, Shannon calls completely random series zero-ordered approximations. Putting randomly selected words after each other yields totally unintelligible lines. As one selects words according to their frequency in a huge corpus, the resulting text gets more natural. If we go further, and we take two-word or three-word or n-word sequences, we get better and better results. One can see these n-word sequences (or n-grams) as transitions from one word to the other.  \n",
    "  \n",
    "This is an example of a text generator using this simple idea.\n",
    "\n",
    "We start by loading some text dataset. Since a Markov generator becomes interesting only with a pretty large corpus, we use a couple of Shakespeare's works.  \n",
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f0d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18e0843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 20907\n",
      "Sample line at position 0 THE TEMPEST\n",
      "Sample line at position 999 Who shall be of as little memory\n"
     ]
    }
   ],
   "source": [
    "dirname = '../datasets/shakespeare/'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            \n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                # append it to the list\n",
    "                lines.append(pure_line)\n",
    "                \n",
    "n_lines = len(lines)\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c77fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALIBAN\tAs wicked dew as e'er my mother brush'd\n",
      "With raven's feather from unwholesome fen\n",
      "Drop on you both! a south-west blow on ye\n",
      "And blister you all o'er!\n",
      "PROSPERO\tFor this, be sure, to-night thou shalt have cramps,\n",
      "Side-stitches that shall pen thy breath up; urchins\n",
      "Shall, for that vast of night that they may work,\n",
      "All exercise on thee; thou shalt be pinch'd\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(lines[506:514]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa76324",
   "metadata": {},
   "source": [
    "A total of 20K lines have been read and they form the input corpus.  \n",
    "### Clean the data\n",
    "For simplicity, we are going to clean them from some punctuation and symbols and also lowercase everything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32062eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 20907\n",
      "Sample line at position 0 the tempest\n",
      "Sample line at position 999 who shall be of as little memory\n"
     ]
    }
   ],
   "source": [
    "# go through each line\n",
    "for i, line in enumerate(lines):\n",
    "    # convert to all lowercase\n",
    "    lines[i] = line.lower()\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b017293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(hdln):\n",
    "    hdln = hdln.replace(\"'\", \"\")\n",
    "    hdln = hdln.replace('\"', \"\")\n",
    "    hdln = hdln.replace(\"…\", \"\")\n",
    "    hdln = hdln.replace(\"”\", \"\")\n",
    "    hdln = hdln.replace(\"`\", \"\")\n",
    "    hdln = hdln.replace(\"’\", \"\")\n",
    "    hdln = hdln.replace(\"“\", \"\")\n",
    "    hdln = hdln.replace(\"(\", \"\")\n",
    "    hdln = hdln.replace(\")\", \"\")\n",
    "    hdln = hdln.replace(\"#\", \"\")\n",
    "    return hdln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f011ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caliban\tas wicked dew as eer my mother brushd\n",
      "with ravens feather from unwholesome fen\n",
      "drop on you both! a south-west blow on ye\n",
      "and blister you all oer!\n",
      "prospero\tfor this, be sure, to-night thou shalt have cramps,\n",
      "side-stitches that shall pen thy breath up; urchins\n",
      "shall, for that vast of night that they may work,\n",
      "all exercise on thee; thou shalt be pinchd\n"
     ]
    }
   ],
   "source": [
    "cleanlines = [e.strip() for e in lines]\n",
    "cleanlines = [e.split(\"|\")[0] for e in cleanlines]\n",
    "cleanlines = [clean(e) for e in cleanlines]\n",
    "print(\"\\n\".join(cleanlines[506:514]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2243f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\\n\".join(cleanlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c73b",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "Next we will extract all tokens from the corpus and we use the old good NLTK tokenizer.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4110df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d206deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "line_length = []\n",
    "\n",
    "for line in cleanlines:\n",
    "    tkns = word_tokenize(line)\n",
    "    line_length.append(len(tkns))\n",
    "    tokens.extend(tkns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50accc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average line length is 8.942268139857465\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average line length is {sum(line_length)/len(line_length)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11da2c1",
   "metadata": {},
   "source": [
    "## Get token frequency\n",
    "To improve the word generation we can use weights based on the token's frequency when randomly picking a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9db79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# a sub-class that is used to count hashable objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0964eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count frequencies\n",
    "token_freq = Counter(tokens)\n",
    "type(token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18d72599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 14890), ('.', 6195), ('the', 4843), ('and', 4499), ('i', 3520)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0004d",
   "metadata": {},
   "source": [
    "A Counter object has key and values: the word itself and its frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05354b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tempest', 'dramatis', 'personae', 'alonso']\n",
      "[4843, 20, 6, 6, 50]\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freq.keys())[:5]) # the first five key,values in the corpus\n",
    "print(list(token_freq.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9b61a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4843),\n",
       " ('tempest', 20),\n",
       " ('dramatis', 6),\n",
       " ('personae', 6),\n",
       " ('alonso', 50)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(token_freq.items())[:5][:5]  # key and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f99c7",
   "metadata": {},
   "source": [
    "## First order text generation\n",
    "Having generated and saved the frequency tables, we have a simple model and we can start working on text generation. Our simple generator takes as arguments the corpus, an initial seed and an order n (how many n-grams to consider).   \n",
    "The first order considers only the tokens and their frequency in the corpus. Each generated token is randomly selected. Also notes that the seed has no impact on the next tokens.  \n",
    "Don't expect good results from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb36c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b582fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialSeed =\"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa5cb2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max',\n",
       " 'the',\n",
       " 'to',\n",
       " 'reeking',\n",
       " 'as',\n",
       " 'day',\n",
       " 'gave',\n",
       " 'it',\n",
       " 'you',\n",
       " 'our',\n",
       " 'same']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genLine = [initialSeed]\n",
    "for i in range(10): # we generate ten tokens randomly but frequency-based   \n",
    "    w = random.choices(population=list(token_freq.keys()),\n",
    "                                  weights=list(token_freq.values()))[0]\n",
    "    genLine.append(w)\n",
    "    \n",
    "genLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe086157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max the to reeking as day gave it you our same\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(item) for item in genLine])) # this is the result, formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334a5e4",
   "metadata": {},
   "source": [
    "This generator is quite poor because the words are not considered together, just random (although based on the frequency, this is why filler and stop words come out often).  \n",
    "## Second-order text generation, using bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fab000",
   "metadata": {},
   "source": [
    "For better results, we can consider bigrams, so two tokens together.  \n",
    "For this we first need to get all possible bigrams and their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "382bc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a574cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "\n",
    "for line in cleanlines:\n",
    "    tkns = word_tokenize(line)\n",
    "    bgrms = list(ngrams(tkns, 2))\n",
    "    bigrams.extend(bgrms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "453a5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e23f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = Counter(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39013484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166090"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_total = sum(bigram_freq.values())\n",
    "bigram_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c58f75b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and'), 971),\n",
       " ((',', 'i'), 492),\n",
       " ((',', 'my'), 478),\n",
       " (('my', 'lord'), 436),\n",
       " (('[', 'enter'), 328)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c44e6",
   "metadata": {},
   "source": [
    "Note that each key is now a tuple formed by two tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b929f56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'tempest'), ('dramatis', 'personae'), ('alonso', 'king'), ('king', 'of'), ('of', 'naples')]\n",
      "[14, 6, 1, 20, 11]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigram_freq.keys())[:5])\n",
    "print(list(bigram_freq.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f03d69af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'tempest'), 14),\n",
       " (('dramatis', 'personae'), 6),\n",
       " (('alonso', 'king'), 1),\n",
       " (('king', 'of'), 20),\n",
       " (('of', 'naples'), 11)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_freq.items())[:5][:5]  # key and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f172b7",
   "metadata": {},
   "source": [
    "The principle of second order generation is slightly different, as we can use the seed to find bigrams with it.  \n",
    "If the seed is not in the corpus, it randomly selects a word, otherwise it returns a weighted choice from the  tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef9158c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_generator_2nd_order(seed, DEBUG=False):\n",
    "    \n",
    "    if seed not in tokens:\n",
    "        seed = random.choice(list(token_freq.keys())) # get another word\n",
    "        if DEBUG:\n",
    "            print(\"New seed is: \", seed)\n",
    "        \n",
    "    bgms = {k: v for (k, v) in bigram_freq.items() if k[0] == seed}\n",
    "    if DEBUG:\n",
    "        print(\"Bigrams are: \", bgms)\n",
    "        \n",
    "    wds = [e[1] for e in bgms.keys()]\n",
    "        \n",
    "    if wds:\n",
    "        weights = [float(e) for e in bgms.values()]\n",
    "        return random.choices(population=wds, weights=weights)[0]\n",
    "    else:\n",
    "        return random.choices(population=list(token_freq.keys()),\n",
    "                                  weights=list(token_freq.values()))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178bfe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max', 'did', 'fight', 'for', 'it', ',', 'scarce', ',', 'right', 'use']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genLine = []\n",
    "genLine.append(initialSeed)\n",
    "for i in range(1,10): # we generate ten tokens   \n",
    "    w = token_generator_2nd_order(genLine[-1]) # pass the last generated word as new seed\n",
    "    genLine.append(w)\n",
    "    \n",
    "genLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f0222b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max did fight for it , scarce , right use\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(item) for item in genLine])) # this is the result, formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14338e",
   "metadata": {},
   "source": [
    "It looks slightly better, I like how the sentence started!  \n",
    "We can further improve by looking at 3 or 4 tokens together.  \n",
    "\n",
    "## N-th order text generation using 3 and 4-grams\n",
    "Again, we first generate the 3 and 4-grams collections, with their frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12471ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "fourgrams = []\n",
    "\n",
    "for line in cleanlines:\n",
    "    tkns = word_tokenize(line)\n",
    "    trgms = list(ngrams(tkns, 3))\n",
    "    trigrams.extend(trgms)\n",
    "    frgrms = list(ngrams(tkns, 4))\n",
    "    fourgrams.extend(frgrms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2218fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_freq = Counter(trigrams)\n",
    "fourgram_freq = Counter(fourgrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58315037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145358"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_total = sum(trigram_freq.values())\n",
    "fourgram_total = sum(fourgram_freq.values())\n",
    "trigram_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45fb8e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124858"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b47169bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'my', 'lord'), 271),\n",
       " (('king', 'richard', 'iii'), 184),\n",
       " (('my', 'lord', ','), 175),\n",
       " ((',', 'sir', ','), 119),\n",
       " (('[', 'exeunt', ']'), 96)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b86cee41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('alonso', 'king', 'of'), 1),\n",
       " (('king', 'of', 'naples'), 5),\n",
       " (('of', 'naples', '.'), 4),\n",
       " (('sebastian', 'his', 'brother'), 1),\n",
       " (('his', 'brother', '.'), 3)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigram_freq.items())[:5][:5]  # key and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "789b9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_generator_nth_order(order, seed, seed2, seed3=None, DEBUG=False):\n",
    "    \n",
    "    if seed not in tokens:\n",
    "        seed = random.choice(list(token_freq.keys()))\n",
    "        if DEBUG:\n",
    "            print(\"New seed is: \", seed)\n",
    "        \n",
    "    if order == 3:\n",
    "                        # we need two words to find the third\n",
    "        tgms = {k: v for (k, v) in trigram_freq.items() if k[:2] == (seed,seed2)}\n",
    "        if DEBUG:\n",
    "            print(\"Trigrams are: \", tgms)\n",
    "            \n",
    "        wds = [e[2] for e in tgms.keys()]\n",
    "        if wds:\n",
    "            weights = [float(e) for e in tgms.values()]\n",
    "            return random.choices(population=wds, weights=weights)[0]\n",
    "        else:\n",
    "            return random.choices(population=list(token_freq.keys()),\n",
    "                                  weights=list(token_freq.values()))[0]\n",
    "        \n",
    "    else:\n",
    "                        # we need three words to find the fourth\n",
    "        ngms = {k: v for (k, v) in fourgram_freq.items() if k[:3] == (seed,seed2,seed3)}\n",
    "        if DEBUG:\n",
    "            print(\"Fourgrams are: \", ngms)\n",
    "            \n",
    "        wds = [e[3] for e in ngms.keys()]\n",
    "        if wds:\n",
    "            weights = [float(e) for e in ngms.values()]\n",
    "            return random.choices(population=wds, weights=weights)[0]\n",
    "        else:\n",
    "            return random.choices(population=list(token_freq.keys()),\n",
    "                                  weights=list(token_freq.values()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20af07e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max',\n",
       " 'of',\n",
       " 'place',\n",
       " 'and',\n",
       " 'duty',\n",
       " 'that',\n",
       " 'i',\n",
       " 'respect',\n",
       " ',',\n",
       " 'for',\n",
       " 'joy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd order text generation\n",
    "\n",
    "genLine = []\n",
    "genLine.append(initialSeed)\n",
    "w2 = token_generator_2nd_order(genLine[-1]) # we need a second word\n",
    "genLine.append(w2)\n",
    "\n",
    "for i in range(1,10): # we generate ten tokens   \n",
    "    w = token_generator_nth_order(3, genLine[-2], genLine[-1]) # pass the last two generated words as new seeds\n",
    "    genLine.append(w)\n",
    "    \n",
    "genLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e7d6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max of place and duty that i respect , for joy\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(item) for item in genLine])) # this is the result, formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3ef9b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max',\n",
       " 'he',\n",
       " ',',\n",
       " ',',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'and',\n",
       " 'second',\n",
       " 'cause',\n",
       " ':',\n",
       " 'i']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4th order text generation\n",
    "\n",
    "genLine = []\n",
    "genLine.append(initialSeed)\n",
    "w2 = token_generator_2nd_order(initialSeed) # now we need a second and third word\n",
    "genLine.append(w2)\n",
    "w3 = token_generator_nth_order(3, initialSeed, w2) \n",
    "genLine.append(w3)\n",
    "\n",
    "for i in range(1,10): # we generate ten tokens   \n",
    "    w = token_generator_nth_order(4, genLine[-3], genLine[-2], genLine[-1]) # pass the last two generated words as new seeds\n",
    "    genLine.append(w)\n",
    "    \n",
    "genLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d59573ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max he , , of the first and second cause : i\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([str(item) for item in genLine])) # this is the result, formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c804eb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Let's put all together in a class. It can be initialised using lines taken from any text, after which will have tokens and ngrams. One method is to generate the next token and then several methods to get sentences.  \n",
    "I introduce also another change: if there is no suitable fourgrams, then it will pick a random word independently from the weights. It adds a bit of unpredictability to the sentence and avoid too many articles and other stop words. It's kind of tuning the temperature of the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c9711c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShannonMarkovTokenGenerator:\n",
    "    def __init__(self, l):\n",
    "        print(\"Generating the model from total lines in input: \", len(l))\n",
    "        lines = [e.split(\"|\")[0] for e in l]\n",
    "            # go through each line and clean it\n",
    "        for i, line in enumerate(lines):\n",
    "            # convert to all lowercase\n",
    "            lines[i] = line.lower()\n",
    "            lines[i] = line.strip()\n",
    "            lines[i] = line.replace(\"'\", \"\")\n",
    "            lines[i] = line.replace('\"', \"\")\n",
    "            lines[i] = line.replace(\"…\", \"\")\n",
    "            lines[i] = line.replace(\"”\", \"\")\n",
    "            lines[i] = line.replace(\"`\", \"\")\n",
    "            lines[i] = line.replace(\"“\", \"\")\n",
    "            lines[i] = line.replace(\"(\", \"\")\n",
    "            lines[i] = line.replace(\"(\", \"\")\n",
    "            lines[i] = line.replace(\"#\", \"\")\n",
    "\n",
    "            # prepare tokens and ngrams\n",
    "        tokens = []\n",
    "        bigrams = []\n",
    "        trigrams = []\n",
    "        fourgrams = []\n",
    "        line_length = []\n",
    "\n",
    "        for line in lines:\n",
    "            tkns = word_tokenize(line)\n",
    "            bgrms = list(ngrams(tkns, 2))\n",
    "            trgms = list(ngrams(tkns, 3))\n",
    "            frgrms = list(ngrams(tkns, 4))\n",
    "            \n",
    "            tokens.extend(tkns)\n",
    "            bigrams.extend(bgrms)\n",
    "            trigrams.extend(trgms)\n",
    "            fourgrams.extend(frgrms)\n",
    "\n",
    "            line_length.append(len(tkns))\n",
    "\n",
    "            # tokens and ngrams are stored\n",
    "        self.corpusTokens = tokens\n",
    "        self.tokensFrequence = Counter(tokens)\n",
    "        self.bigramsFrequence = Counter(bigrams)\n",
    "        self.trigramsFrequence = Counter(trigrams)\n",
    "        self.fourgramsFrequence = Counter(fourgrams)\n",
    "\n",
    "            # let's store also a couple of stats about the text\n",
    "        self.tokensTotal = sum(self.tokensFrequence.values())\n",
    "        self.avgLineLength = sum(line_length)/len(line_length)\n",
    "        self.lineLength = round(self.avgLineLength) + 1\n",
    "\n",
    "        print(\"Done. Total tokens extracted: \", self.tokensTotal)\n",
    "    \n",
    "    \n",
    "    def token_generator(self, seed, order, seed2=None, seed3=None):\n",
    "        '''\n",
    "        generate a word (token) based on an input n-gram and the text stored\n",
    "        order = identifies if bigram, trigram, etc.; number between 1 and 4\n",
    "        seed = first input token\n",
    "        seed2 = optional. second input token\n",
    "        seed3 = optional. third input token\n",
    "        returns a token (string of characters)\n",
    "        '''\n",
    "    \n",
    "            # check that the first seed is in the corpus, otherwise pick randomly a new one\n",
    "            # we assume that the other seeds are coming from the corpus, as they have been previously generated\n",
    "        if seed not in tokens:\n",
    "            seed = random.choice(list(self.tokensFrequence.keys()))\n",
    "            \n",
    "        if (not seed2) & (order > 2):\n",
    "            seed2 = token_generator_2nd_order(seed) # needed only first time\n",
    "  \n",
    "            # Second-order\n",
    "            # select a new token based on the given seed and the built bigrams\n",
    "        if order == 2:\n",
    "            bgms = {k: v for (k, v) in self.bigramsFrequence.items() if k[0] == seed}\n",
    "\n",
    "            wds = [e[1] for e in bgms.keys()]  # list of all possible words\n",
    "                    # if no available words from the bigrams, pick a random one from corpus\n",
    "            if wds:\n",
    "                weights = [float(e) for e in bgms.values()]\n",
    "                return random.choices(population=wds, weights=weights)[0]\n",
    "            else:\n",
    "                return random.choices(population=list(self.tokensFrequence.keys()),\n",
    "                                  weights=list(self.tokensFrequence.values()))[0]\n",
    "            # Third-order\n",
    "            # select a new token based on the given two seeds and the built trigrams\n",
    "        elif order == 3:\n",
    "\n",
    "            tgms = {k: v for (k, v) in trigram_freq.items() if k[:2] == (seed,seed2)}\n",
    "            wds = [e[2] for e in tgms.keys()]\n",
    "            if wds:\n",
    "                weights = [float(e) for e in tgms.values()]\n",
    "                return random.choices(population=wds, weights=weights)[0]\n",
    "            else:\n",
    "                return random.choice(list(self.tokensFrequence.keys()))\n",
    "        \n",
    "        else:\n",
    "            ngms = {k: v for (k, v) in fourgram_freq.items() if k[:3] == (seed,seed2,seed3)}\n",
    "            wds = [e[3] for e in ngms.keys()]\n",
    "            if wds:\n",
    "                weights = [float(e) for e in ngms.values()]\n",
    "                return random.choices(population=wds, weights=weights)[0]\n",
    "            else:\n",
    "                return random.choice(list(self.tokensFrequence.keys()))  # no more weights!\n",
    "\n",
    "\n",
    "\n",
    "    def generate1stOrderLine(self, nLines=1):\n",
    "        '''\n",
    "        generate one or more line of text (string) based on the text stored\n",
    "        nLines = optional. How many lines to generate. Default = 1\n",
    "        returns a string: nLines (separated by newline) of averageLineLength words\n",
    "        '''\n",
    "        \n",
    "        genLines = \"\"\n",
    "        for l in range(nLines):\n",
    "            genLine = []\n",
    "            for i in range(self.lineLength):    \n",
    "                w = random.choices(population=list(self.tokensFrequence.keys()),\n",
    "                                  weights=list(self.tokensFrequence.values()))[0]\n",
    "                genLine.append(w)\n",
    "        \n",
    "            strLine = ' '.join([str(item) for item in genLine]) # make the list of words a string\n",
    "            genLines += strLine \n",
    "            genLines += \"\\n\"\n",
    "        \n",
    "        return genLines\n",
    "\n",
    "\n",
    "    def generate2ndOrderLine(self, seed, nLines=1):            \n",
    "        '''\n",
    "        generate one or more line of text (string) based on an initial seed and the text stored, using bigrams\n",
    "        seed = first input token\n",
    "        nLines = optional. How many lines to generate. Default = 1\n",
    "        returns a string: nLines (separated by newline) of averageLineLength words\n",
    "        '''\n",
    "        if seed not in tokens:\n",
    "            seed = random.choice(list(self.tokensFrequence.keys()))\n",
    "        w2 = self.token_generator(seed, 2) # first word from seed\n",
    "                    \n",
    "        genLine = []\n",
    "        genLine.append(seed)    # start the line with the seed and the first word            \n",
    "        genLine.append(w2)\n",
    "        genLines = seed + \" \" + w2 + \" \"\n",
    "\n",
    "        for l in range(nLines):\n",
    "            \n",
    "            for i in range(1, self.lineLength):  \n",
    "\n",
    "                w = self.token_generator(genLine[-2], 2, genLine[-1])\n",
    "\n",
    "                genLine.append(w)\n",
    "        \n",
    "            strLine = ' '.join([str(item) for item in genLine[2:]])\n",
    "            genLines += strLine \n",
    "            genLines += \"\\n\"\n",
    "            \n",
    "            # we keep only the last two words for next line\n",
    "            genLine = genLine[-2:]\n",
    "        \n",
    "        return genLines\n",
    "\n",
    "    def generate3rdOrderLine(self, seed, nLines=1):            \n",
    "        '''\n",
    "        generate one or more line of text (string) based on an initial seed and the text stored, using trigrams\n",
    "        seed = first input token\n",
    "        nLines = optional. How many lines to generate. Default = 1\n",
    "        returns a string: nLines (separated by newline) of averageLineLength words\n",
    "        '''\n",
    "        if seed not in tokens:\n",
    "            seed = random.choice(list(self.tokensFrequence.keys()))\n",
    "        w2 = self.token_generator(seed, 2) # first word from seed\n",
    "        w3 = self.token_generator(seed, 3, w2) # second word from the two above\n",
    "                    \n",
    "        genLine = []\n",
    "        genLine.append(seed)        \n",
    "        genLine.append(w2) \n",
    "        genLine.append(w3)\n",
    "\n",
    "        genLines = seed + \" \" + w2 + \" \" + w3 + \" \"\n",
    "\n",
    "        for l in range(nLines):\n",
    "            \n",
    "            for i in range(1, self.lineLength):  \n",
    "                w = self.token_generator(genLine[-3], 3, genLine[-2], genLine[-1])\n",
    "\n",
    "                genLine.append(w)\n",
    "        \n",
    "            strLine = ' '.join([str(item) for item in genLine[3:]])\n",
    "            genLines += strLine \n",
    "            genLines += \"\\n\"\n",
    "\n",
    "                        # we keep only the last three words for next line\n",
    "            genLine = genLine[-3:]\n",
    "\n",
    "        return genLines\n",
    "    \n",
    "    \n",
    "    def generate4thOrderLine(self, seed, nLines=1):       \n",
    "        '''\n",
    "        generate one or more line of text (string) based on an initial seed and the text stored, using fourgrams\n",
    "        seed = first input token\n",
    "        nLines = optional. How many lines to generate. Default = 1\n",
    "        returns a string: nLines (separated by newline) of averageLineLength words\n",
    "        '''\n",
    "            \n",
    "        if seed not in tokens:\n",
    "            seed = random.choice(list(self.tokensFrequence.keys()))\n",
    "        w2 = self.token_generator(seed, 2) # first word\n",
    "        w3 = self.token_generator(seed, 3, w2) # second word \n",
    "                    \n",
    "        genLine = []\n",
    "        genLine.append(seed)        \n",
    "        genLine.append(w2) \n",
    "        genLine.append(w3)\n",
    "\n",
    "        genLines = seed + \" \" + w2 + \" \" + w3 + \" \"\n",
    "\n",
    "        for l in range(nLines):\n",
    "            \n",
    "            for i in range(1, self.lineLength):  \n",
    "                w = self.token_generator(genLine[-3], 4, genLine[-2], genLine[-1])\n",
    "\n",
    "                genLine.append(w)\n",
    "        \n",
    "            strLine = ' '.join([str(item) for item in genLine[3:]])\n",
    "            genLines += strLine \n",
    "            genLines += \"\\n\"\n",
    "\n",
    "                        # we keep only the last two words for next line\n",
    "            genLine = genLine[-3:]\n",
    "\n",
    "        return genLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1d8ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the model from total lines in input:  20907\n",
      "Done. Total tokens extracted:  190812\n"
     ]
    }
   ],
   "source": [
    "model = ShannonMarkovTokenGenerator(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "177d9b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average line length is 9.1267039747453\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average line length is {model.avgLineLength}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e869c",
   "metadata": {},
   "source": [
    "You can access directly the tokens and their frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea998c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 14890), ('.', 6196), ('the', 4836), ('and', 4499), ('i', 3871)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokensFrequence.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee77dfb",
   "metadata": {},
   "source": [
    "And now we generate the different orders lines, two lines each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5372e666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how devise swear bless too been is traitors yet themselves\n",
      ", sight work lord . fertile forth for had in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate1stOrderLine(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7abc20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how is't thou with hast me enchanted my her greatness ,\n",
      "will is receive her them sorrow wear and thou\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate2ndOrderLine(\"how\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5d3e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how is it it out accommodation . weigh'st begone indignation sparingly seest\n",
      "re-deliver wheat pain winters twenty skyish wave-worn incharitable thanes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate3rdOrderLine(\"how\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b45e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how satisfied , my lord , what is your tidings ? commencing\n",
      "whiff daylight stripping entertainer norways unswear antres brakenbury fulsome\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generate4thOrderLine(\"how\", 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b91f1",
   "metadata": {},
   "source": [
    "Note that now the 3rd and 4th order get more random words and less articles.  \n",
    "That's all. Feel free to try with different corpus of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
