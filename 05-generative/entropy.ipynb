{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2a3fd4",
   "metadata": {},
   "source": [
    "# Shannon's entropy\n",
    "In information theory, the entropy of a random variable is the average level of \"information\" inherent to the variable's possible outcomes. \n",
    "\n",
    "The American mathematician\n",
    "Claude Elwood Shannon, was interested in how\n",
    "to mathematically model the process of\n",
    "information transmission.\n",
    " \n",
    "One of the main problems\n",
    "that Shannon encountered was how to measure the information\n",
    "content of a message. He published his\n",
    "findings in a 1948 article, \"A Mathematical Theory\n",
    "of Communication\" which has become one of the most cited\n",
    "scientific works in history and founded the field of information theory.  \n",
    "\n",
    "Shannon realized that the information content of a message or of any random variable is best thought of as **how\n",
    "surprising its content is, how much uncertainty\n",
    "it resolves.** As the basic measure of\n",
    "information content, he introduced the unit bit.\n",
    " \n",
    "If you want to capture the\n",
    "information content of a random variable with N different outcomes that\n",
    "are all equally likely, how many bits does it take? Shannon showed that the answer is given by the\n",
    "l**ogarithm of N with base 2**. At the suggestion of John von Neumann, he called\n",
    "his measure entropy following the notion of\n",
    "entropy that Boltzmann had introduced in\n",
    "thermodynamics and physics. Entropy is denoted with the\n",
    "uppercase Greek letter Eta, which is written like\n",
    "the English letter H.  \n",
    "  \n",
    "**H = log2(N)**   \n",
    "  \n",
    "Let's see some example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0679b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16980b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(1,2) #log base 2 of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bc023",
   "metadata": {},
   "source": [
    "If the outcome of a random variable can only take on one possible value\n",
    "with probability one, then you will not\n",
    "learn anything new. You already know the outcome\n",
    "it was going to take. So the information content of\n",
    "that random variable is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f4fd68",
   "metadata": {},
   "source": [
    "## Entropy formula\n",
    "We can encapsulate the formula into an easier Python function which requires only the number of possible outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898238ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy (n, logbase=2):\n",
    "    # Calculate the Shannon Entropy given random variable with equally likely outcomes\n",
    "    # n = number of possible outcomes of the random variable; n > 0\n",
    "    # logbase = base of the logarithm; default is 2 (for digital bits)\n",
    "    # returns the Shannon entropy H\n",
    "    \n",
    "    return log(n,logbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a510362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1) # same as log(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a02cb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabef85b",
   "metadata": {},
   "source": [
    "The information content of a random variable\n",
    "that can take on two outcomes with\n",
    "equal probability (this could be e.g. the outcome of a fair coin toss) can be captured in one bit.  \n",
    "So you can understand the basic unit of Shannon's entropy as the information contained in the outcome of a coin toss.    \n",
    "  \n",
    "And so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fdcb80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(4) # entropy of a variable with four possible outcomes is two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a39da",
   "metadata": {},
   "source": [
    "Note that if H=log2(N) then N = 2^H  \n",
    "That means 1 bit can contain all the information needed for a toss coin outcome.  \n",
    "  \n",
    "This can be used to claculate how many bits are necessary to contain an event information.  \n",
    "For example, 2 bits can contain / report all the information needed for an event with 4 possible outcomes.  \n",
    "## Fractional bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce2fbd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3219280948873626"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(10) # entropy of a variable with 10 possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ef66f",
   "metadata": {},
   "source": [
    "The information content captured by one digit in our\n",
    "decimal system - meaning any digit from 0 to 9 - is the base 2 logarithm\n",
    "of 10 and is 3.32.  \n",
    "How should we think\n",
    "of fractional bits?  \n",
    "Well, one way of thinking\n",
    "about it is that if I have a long sequence\n",
    "of decimal digits, I can piece together\n",
    "fractional bits to express the information\n",
    "content of the entire sequence.  \n",
    "  \n",
    "So for example, if I have\n",
    "three decimal digits, with them I can capture any\n",
    "number from 0 to 999.  \n",
    "And the information content is the\n",
    "base 2 logarithm of 1000 or 3 times the base 2 logarithm\n",
    "of 10, which is 9.97.   \n",
    "  \n",
    "So I can encode a three-digit\n",
    "decimal number by 10 bits.  \n",
    "In fact, 10 bits could\n",
    "capture up to 2^10, which is 1,024\n",
    "different numbers (sounds familiar?). So there is 0.03 bits\n",
    "of redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1427b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.965784284662087"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037033d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e094de",
   "metadata": {},
   "source": [
    "## Entropy formula given (equal) probability \n",
    "For a random variable *x* with equi-probable outcomes, the formula can also be written as:  \n",
    "**H = log N = -log P** where P is the probability P = 1/N (for equi-probable outcomes)  \n",
    "For example, instead of log2(10) i.e., using 10 possible outcomes, we could use the probability, which would be 1/10 and get the same entropy value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67d4a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3219280948873626"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "613934a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.321928094887362"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-log(1/10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6bd6a2",
   "metadata": {},
   "source": [
    "Therefore, we can rewrite the function accepting the probability as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96777fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyP (P, logbase=2):\n",
    "    # Calculate the Shannon Entropy given random variable with equally likely outcomes\n",
    "    # P is the probablity of each outcome, p>0\n",
    "    return -log(P,logbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e88dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.321928094887362"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropyP(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c92396d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropyP(0.5)  #same as entropy (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aad8ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e08b4",
   "metadata": {},
   "source": [
    "### Extra 1: other logbase\n",
    "Usually log2 is used in entropy and that implies the unit of information content are bits, which has advantages when talking about information managed by computers.  \n",
    "Anyway we could use other logarithm bases; if we use base 10 we would get a measure that tells us how many decimal digits we need to capture the same information.  \n",
    "  \n",
    "  As an example:  \n",
    "we have seen that we need at least 4 bits (entropy=3.32) to represent the information of a decimal number from 0 to 9 but we would need only 1 decimal number to represent it.  \n",
    "In fact log10(10)=1 and the entropy (using base 10) of 10 is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa9d0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3219280948873626"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(10) # standard definition: use base 2 and bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6094f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(10,10) # entropy using decimal digits instead of bits: only 1 decimal digit needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f685ece",
   "metadata": {},
   "source": [
    "### Extra2: Entropy for not equal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb5b50",
   "metadata": {},
   "source": [
    "Let's generalize this a bit, and let's assume that we have a random variable *x* with\n",
    "N different outcomes that have different\n",
    "probabilities Pi, that are all non-negative and add up to one.  \n",
    "Then we can denote the Shannon information\n",
    "content, or the surprisal, of a given outcome *i* as\n",
    "minus the logarithm of Pi.  \n",
    "  \n",
    "If the probability of\n",
    "outcome *i* is close to one, then we are not very\n",
    "surprised by that outcome and the information content, or surprisal, is close to zero. On the other hand, if the probability of\n",
    "an outcome is very low, then we will be very surprised, so the information content of that outcome is a large number.  \n",
    "This should make\n",
    "the definition of information content intuitive.  \n",
    "  \n",
    "  Let's see an example: how different is the entropy for a content with five possible outcomes that do not have the same probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dacda456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.321928094887362"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(5) # this is the entropy if all outcomes have the same probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fc031",
   "metadata": {},
   "source": [
    "Now we make a list of five different probabilities, each one should represent the probability of outcome *i*, and sum up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484052f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDiffProb = [0.1, 0.2, 0.4, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f017b2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(exampleDiffProb)  # expected to be 1.0, give or take a floating-point error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1abc8f",
   "metadata": {},
   "source": [
    "We can calculate the single entropies *Hi*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef9805a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33219280948873625,\n",
       " 0.46438561897747244,\n",
       " 0.5287712379549449,\n",
       " 0.46438561897747244,\n",
       " 0.33219280948873625]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = [p * entropyP(p) for p in exampleDiffProb]\n",
    "ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8550183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for not equal probabilities =  2.121928094887362\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy for not equal probabilities = \", sum(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa347bc0",
   "metadata": {},
   "source": [
    "Telling us the entropy is ~2.12 bits  \n",
    "  \n",
    "Note that it's lower. Now let's see a more interesting example: the entropy of a die roll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "415539f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(6) # die has 6 possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0c2fa",
   "metadata": {},
   "source": [
    "We can assume that a fair die has all equal probabilities for the outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4bf9cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairDie = [1/6 for x in range(6)]\n",
    "fairDie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad20d",
   "metadata": {},
   "source": [
    "What happens if it's an unfair die, so the probability to get a 6 is higher than the others?  \n",
    "Let's modifiy the probabilities list and calculate its entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40650b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06666666666666665,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.16666666666666666,\n",
       " 0.26666666666666666]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfairDie = fairDie\n",
    "unfairDie[0] = fairDie[0] - 0.1\n",
    "unfairDie[5] = fairDie[5] + 0.1\n",
    "unfairDie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b24aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(unfairDie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa1632b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for not equal probabilities =  2.49227186568361\n"
     ]
    }
   ],
   "source": [
    "ents = [p * entropyP(p) for p in unfairDie]\n",
    "print(\"Entropy for not equal probabilities = \", sum(ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca749ba1",
   "metadata": {},
   "source": [
    "The entropy for the unfair die is lower!  \n",
    "Here is a general rule.  \n",
    "The entropy is maximized when all outcomes are equally likely: **if all P(xi) values are equal, the entropy is at its maximum, indicating maximum uncertainty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c6142",
   "metadata": {},
   "source": [
    "## Information as decrease in uncertainty\n",
    "Shannon observed that\n",
    "information = a decrease in uncertainty.  \n",
    "Given our measure of entropy, the information obtained from a signal or message is captured by:  \n",
    "R = H_before - H_after (R stands for Ratio)  \n",
    "  \n",
    "Example: if you roll a die and the result is \"the number is even\" then the information obtained is:    \n",
    "R = H(6) - H(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4920382d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy before, as the outcome can be any number between 1 and 6\n",
    "entropy(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e40afc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5849625007211563"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#entropy after, now we know it's even: can only be 2, 4 or 6 (one of 3 numbers)\n",
    "entropy(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c33b1c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(6)-entropy(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caaca3b",
   "metadata": {},
   "source": [
    "The information obtained is 1 bit, which is exactly the information of knowing if a die roll is odd or even:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e17e69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the entropy = information of knowing if it's odd or even\n",
    "entropy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025cc13",
   "metadata": {},
   "source": [
    "## Joint entropy\n",
    "Consider two random variables x and y, then the Joint entropy  \n",
    "H (x,y) = −Sum(Pij * logPij)  \n",
    "  \n",
    "If x and y are independent, then Pij = Pi * Pj and so:  \n",
    "H (x , y ) = H (x ) + H (y )  \n",
    "  \n",
    "Here is an example.  \n",
    "Consider a random variable x with an entropy of 5 bits.  \n",
    "Now let's consider another random variable y = +- square root(x) that is either plus or minus the square root of x with probability 50/50. What is the entropy of y?  \n",
    "This is a case of joint entropy and it's the sum:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "190e967c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 + entropyP(0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13880610",
   "metadata": {},
   "source": [
    "The additional uncertainty about the sign adds one bit of entropy.  \n",
    "Note that how is actually working the variable y (square root or whatever function of x) it doesn't matter. For the entropy it matters only the outcome probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092ae5e",
   "metadata": {},
   "source": [
    "Another example. Imagine you play a coin toss game in which you count how long it takes until you hit the first tail. What is the surprisal of the outcome \"hitting a tail on the third toss\"?  \n",
    "They are all independent. From classical probability theory, we know this is probability 1/2 * 1/2 * 1/2 = 1/8  \n",
    "So its entropy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c44f6665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropyP(1/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61b8f6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to get this is to use the joint entropy of three coin toss games: \n",
    "entropyP(0.5) + entropyP(0.5) + entropyP(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2861fa3",
   "metadata": {},
   "source": [
    "## Conditional entropy\n",
    "Consider two random variables x and y, then the Conditional entropy of x given y is:    \n",
    "H (x|y) = −sum(Pij * logP (x|y) )  \n",
    "  \n",
    "This is equal to the average uncertainty about x if y is known.    \n",
    "We can also express it as the difference between the joint entropy and the entropy of y:  \n",
    "H(x|y) = H(x,y) − H(y)  \n",
    "  \n",
    "The intuition is that if you already know y then the additional information captured by the joint distribution of the two variables is simply the information in x conditional of knowing y.  \n",
    "  \n",
    "An example.  \n",
    "Let x be the outcome of rolling a die and y a binary variable that indicates whether the outcome is odd or even.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ba1d2",
   "metadata": {},
   "source": [
    "What is the conditional entropy of x (roll die) given y (odd or even)?  \n",
    "H(x|y) = H(x,y) - H(y)  \n",
    "H(x,y) = H(x) because in this case x (roll die) contains all the information in y (odd or even)   \n",
    "so the conditional entropy  \n",
    "H(x|y) = H(x) - H(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "415dd485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.584962500721156"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(6) - entropy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deba9db",
   "metadata": {},
   "source": [
    "What is the conditional entropy of y given x (for the roll die)?  \n",
    "The answer is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8274b5",
   "metadata": {},
   "source": [
    "## Mutual information\n",
    "The mutual information I of\n",
    "xy captures the amount of information that two random\n",
    "variables have in common, meaning how much information we can learn\n",
    "about one variable by observing another. It's a symmetric measure, so I of xy equals I of yx.  \n",
    "If the two random\n",
    "variables are independent, then, of course, the mutual\n",
    "information is zero.  \n",
    "If the two variables\n",
    "are precisely the same, then the mutual information equals\n",
    "the entropy of the variable itself, meaning that all the information\n",
    "content is mutual.  \n",
    "  \n",
    "We can use the conditional entropy to\n",
    "write the mutual information of x and y as the information content\n",
    "of x minus the information content of x given that we\n",
    "already know y:  \n",
    "I(x,y) = H(x) - H(x|y)  \n",
    "In other words, knowing y allows us to receive\n",
    "I of xy bits in encoding x.  \n",
    "Being a mutual and symmetric measure, the other way is also valid:  \n",
    "I(x,y) = H(y) - H(y|x)  \n",
    "  \n",
    "  Let's continue our example of the die. X is the roll of the die and y is if odd or even.\n",
    "We have just seen that conditional entropy H(x|y) = 1.58  \n",
    "Then I(x,y) = H(x) - H(x|y) = H(x) - 1.58  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379f3299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(6) # this is H(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84dc651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.58 - 1.58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d4419",
   "metadata": {},
   "source": [
    "In this particular example, all of\n",
    "the content of y is mutual to x and y because when we know x - \n",
    "what number has rolled - we would immediately know\n",
    "y - whether it's even or odd.  \n",
    "Therefore, the mutual information\n",
    "content equals the entropy, or information content, of y - whether it's\n",
    "even or odd - which is precisely one bit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b84adae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
